# -*- coding: utf-8 -*-
"""Project_Co2_modelling_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D0UoYyFtlX2y0anU1I_QwKYTEG-2VE3F

---


# **Modelling**


---

We will train the following models to determine which model is best suited for our problematic.

**Regression:**
*   Linear Regression
*   XGBoost
*   Decision Tree
*   ElasticNet

**Classification:**
*   Logistic Regression
*   XGBoost
*   Decision Tree
*   KNN
"""

""" 
from google.colab import drive
drive.mount('/content/drive') 
"""

"""# **Import libraries**"""

import os
default_n_threads = 8
os.environ['OPENBLAS_NUM_THREADS'] = f"{default_n_threads}"
os.environ['MKL_NUM_THREADS'] = f"{default_n_threads}"
os.environ['OMP_NUM_THREADS'] = f"{default_n_threads}"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import ElasticNet, LogisticRegression, LinearRegression
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import classification_report, f1_score, accuracy_score, mean_squared_error, r2_score, confusion_matrix
from imblearn.metrics import classification_report_imbalanced
from xgboost import XGBClassifier, plot_importance, XGBRegressor
from sklearn import neighbors
import joblib

import warnings
warnings.filterwarnings('ignore')


# Reading data
print("Reading the data is started!")
df = pd.read_csv("../../../data_preprocessed.csv",index_col=0)

print("df length (Should be 3496097):",len(df))

"""# **Encoding Target**"""

# Change the name of target column from "Ewltp (g/km)" to target
df = df.rename(columns={"Ewltp (g/km)":"target"})

# Encoding Target

# Target for 7-class classification
bin = [0, 120, 140, 155, 170, 190, 225, df["target"].max()]
df["target_clf"] = pd.cut(df["target"], bin)
# 'A' = 0,'B'=1,'C'=2, 'D'=3, 'E'=4, 'F'=5, 'G'=6
LE = LabelEncoder()
df["target_clf"] = LE.fit_transform(df["target_clf"])

print("Counts for 7 classes: ")
print(df["target_clf"].value_counts())

"""# **Splitting data into train, test and validation sets**"""

X, X_valid, y_reg, y_valid_reg, y_clf, y_valid_clf = train_test_split(df.drop(["target", "target_clf"], axis=1),
                                                                      df["target"],
                                                                      df["target_clf"],
                                                                      stratify=df["target_clf"],
                                                                      test_size=0.1,
                                                                      random_state=42)
X_train, X_test, y_train_reg, y_test_reg, y_train_clf, y_test_clf = train_test_split(X,
                                                                                     y_reg,
                                                                                     y_clf,
                                                                                     stratify=y_clf,
                                                                                     test_size=0.2,
                                                                                     random_state=42)





"""# **Data normalization**"""

# Scaling data (only numerical features)
numerical_features = ["m (kg)","ep (KW)","Erwltp (g/km)","Fuel consumption"]
scaler = StandardScaler()
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])
X_valid[numerical_features] = scaler.transform(X_valid[numerical_features])

joblib.dump(scaler, 'scaler')


# Storing data
X_test.columns = df.drop(["target", "target_clf"], axis=1).columns
y_test_reg.columns = "target"
y_test_clf.columns = "target_clf"

# X_train.to_csv("X_train.csv")
X_test.iloc[:int(0.7*len(X_test))].to_csv("X_test.csv")
# X_valid.to_csv("X_valid.csv")

# y_train_reg.to_csv("y_train_reg.csv")
y_test_reg.iloc[:int(0.7*len(X_test))].to_csv("y_test_reg.csv")
# y_valid_reg.to_csv("y_valid_reg.csv")

# y_train_clf.to_csv("y_train_clf.csv")
y_test_clf.iloc[:int(0.7*len(X_test))].to_csv("y_test_clf.csv")
# y_valid_clf.to_csv("y_valid_clf.csv")


"""# **Models for Regression**"""

# Initializing DataFrame for comparing results
metrics_compare = []

"""# **Linear Regression**"""

# Multivar Regression
model_name = "LinearRegression"

lr = LinearRegression()
lr.fit(X_train, y_train_reg)

# Predictions
y_train_pred = lr.predict(X_train)
y_test_pred = lr.predict(X_test)

# Compute R2
score_train = lr.score(X_train,y_train_reg)
score_test = lr.score(X_test,y_test_reg)

# Compute RMSE
train_rmse = np.sqrt(mean_squared_error(y_train_reg, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test_reg, y_test_pred))

# Adding scores
metrics_compare.append([model_name,"Train","R^2",round(score_train,4)])
metrics_compare.append([model_name,"Test","R^2",round(score_test,4)])
metrics_compare.append([model_name,"Train","RMSE",round(train_rmse,4)])
metrics_compare.append([model_name,"Test","RMSE",round(test_rmse,4)])

# save model for future use
filename = "Models/Regression/" + model_name + ".sav"
joblib.dump(lr, filename)


"""# **Elastic Net**

Default values:
*   alpha=1.0
*   l1_ratio=0.5
*   fit_intercept=True
*   precompute=False
*   max_iter=1000
*   copy_X=True
*   tol=0.0001
*   warm_start=False
*   positive=False
*   random_state=None
*   selection='cyclic'
"""

# Elastic Net
model_name = "ElasticNet"

enReg = ElasticNet()
enReg.fit(X_train,y_train_reg)

# Predictions
y_train_pred = enReg.predict(X_train)
y_test_pred = enReg.predict(X_test)

# Compute R2
score_train = enReg.score(X_train,y_train_reg)
score_test = enReg.score(X_test,y_test_reg)

# Compute RMSE
train_rmse = np.sqrt(mean_squared_error(y_train_reg, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test_reg, y_test_pred))

# Adding scores
metrics_compare.append([model_name,"Train","R^2",round(score_train,4)])
metrics_compare.append([model_name,"Test","R^2",round(score_test,4)])
metrics_compare.append([model_name,"Train","RMSE",round(train_rmse,4)])
metrics_compare.append([model_name,"Test","RMSE",round(test_rmse,4)])

# save model for future use
filename = "Models/Regression/" + model_name + ".sav"
joblib.dump(enReg, filename)

"""# **Decision Tree Regressor**"""

# DecisionTreeRegressor
model_name = "DecisionTreeRegressor"

dtReg = DecisionTreeRegressor(max_depth = 5)
dtReg.fit(X_train, y_train_reg)

# Predictions
y_train_pred = dtReg.predict(X_train)
y_test_pred = dtReg.predict(X_test)

# Compute R2
score_train = dtReg.score(X_train,y_train_reg)
score_test = dtReg.score(X_test,y_test_reg)

# Compute RMSE
train_rmse = np.sqrt(mean_squared_error(y_train_reg, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test_reg, y_test_pred))

# Adding scores
metrics_compare.append([model_name,"Train","R^2",round(score_train,4)])
metrics_compare.append([model_name,"Test","R^2",round(score_test,4)])
metrics_compare.append([model_name,"Train","RMSE",round(train_rmse,4)])
metrics_compare.append([model_name,"Test","RMSE",round(test_rmse,4)])

# save model for future use
filename = "Models/Regression/" + model_name + ".sav"
joblib.dump(dtReg, filename)

"""# **XGBoost Regressor**"""

# XGBRegressor
model_name = "XGBRegressor"

xgbReg = XGBRegressor(objective='reg:squarederror')
xgbReg.fit(X_train,y_train_reg)

# Predictions
y_train_pred = xgbReg.predict(X_train)
y_test_pred = xgbReg.predict(X_test)

# Compute R2
score_train = r2_score(y_train_reg,y_train_pred)
score_test = r2_score(y_test_reg,y_test_pred)

# Compute RMSE
train_rmse = np.sqrt(mean_squared_error(y_train_reg, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test_reg, y_test_pred))

# Adding scores
metrics_compare.append([model_name,"Train","R^2",round(score_train,4)])
metrics_compare.append([model_name,"Test","R^2",round(score_test,4)])
metrics_compare.append([model_name,"Train","RMSE",round(train_rmse,4)])
metrics_compare.append([model_name,"Test","RMSE",round(test_rmse,4)])

# save model for future use
filename = "Models/Regression/" + model_name + ".sav"
joblib.dump(xgbReg, filename)

"""# **Models for Classification**

# **Logistic Regression**
"""

# LogisticRegression
model_name = "LogisticRegression"

lrClf = LogisticRegression(solver="sag", max_iter = 1000)
lrClf.fit(X_train,y_train_clf)

# Predictions
y_train_pred = lrClf.predict(X_train)
y_test_pred = lrClf.predict(X_test)

# Compute accuracy
score_train = lrClf.score(X_train,y_train_clf)
score_test = lrClf.score(X_test,y_test_clf)

# Compute F1
train_f1=f1_score(y_train_clf,y_train_pred,average="weighted")
test_f1=f1_score(y_test_clf,y_test_pred,average="weighted")

# Adding scores
metrics_compare.append([model_name,"Train","Accuracy",round(score_train,4)])
metrics_compare.append([model_name,"Test","Accuracy",round(score_test,4)])
metrics_compare.append([model_name,"Train","F1",round(train_f1,4)])
metrics_compare.append([model_name,"Test","F1",round(test_f1,4)])

# save model for future use
filename = "Models/Classification/" + model_name + ".sav"
joblib.dump(lrClf, filename)

"""# **K-Nearest Neighbors (KNN)**"""

# KNeighborsClassifier
model_name = "KNeighborsClassifier"

knn = neighbors.KNeighborsClassifier(n_neighbors=3, metric='minkowski')
knn.fit(X_train, y_train_clf)

# Predictions
y_train_pred = knn.predict(X_train)
y_test_pred = knn.predict(X_test)

# Compute accuracy
score_train = knn.score(X_train,y_train_clf)
score_test = knn.score(X_test,y_test_clf)

# Compute F1
train_f1=f1_score(y_train_clf,y_train_pred,average="weighted")
test_f1=f1_score(y_test_clf,y_test_pred,average="weighted")

# Adding scores
metrics_compare.append([model_name,"Train","Accuracy",round(score_train,4)])
metrics_compare.append([model_name,"Test","Accuracy",round(score_test,4)])
metrics_compare.append([model_name,"Train","F1",round(train_f1,4)])
metrics_compare.append([model_name,"Test","F1",round(test_f1,4)])

# save model for future use
filename = "Models/Classification/" + model_name + ".sav"
joblib.dump(knn, filename, compress=3)

"""# **Decision Tree Classifier**"""

# DecisionTreeClassifier
model_name = "DecisionTreeClassifier"

dt_clf = DecisionTreeClassifier(criterion ='entropy', max_depth=4, random_state=123)
dt_clf.fit(X_train, y_train_clf)

# Predictions
y_train_pred = dt_clf.predict(X_train)
y_test_pred = dt_clf.predict(X_test)

# Compute accuracy
score_train = dt_clf.score(X_train,y_train_clf)
score_test = dt_clf.score(X_test,y_test_clf)

# Compute F1
train_f1=f1_score(y_train_clf,y_train_pred,average="weighted")
test_f1=f1_score(y_test_clf,y_test_pred,average="weighted")

# Adding scores
metrics_compare.append([model_name,"Train","Accuracy",round(score_train,4)])
metrics_compare.append([model_name,"Test","Accuracy",round(score_test,4)])
metrics_compare.append([model_name,"Train","F1",round(train_f1,4)])
metrics_compare.append([model_name,"Test","F1",round(test_f1,4)])

# save model for future use
filename = "Models/Classification/" + model_name + ".sav"
joblib.dump(dt_clf, filename)

"""# **XGBoost Classifier**

Default values:

*   max_depth=3
*   learning_rate=0.1
*   n_estimators=100
*   silent=True
*   booster='gbtree'
*   n_jobs=1
*   nthread=None
*   gamma=0
*   min_child_weight=1
*   max_delta_step=0
*   subsample=1
*   colsample_bytree=1
*   colsample_bylevel=1
*   reg_alpha=0
*   reg_lambda=1
*   scale_pos_weight=1
*   base_score=0.5
*   random_state=0
*   seed=None
*   missing=None
"""

# XGBClassifier
model_name = "XGBClassifier"

xgbClf = XGBClassifier(objective='multi:softmax', num_class=7)
xgbClf.fit(X_train,y_train_clf)

# Predictions
y_train_pred = xgbClf.predict(X_train)
y_test_pred = xgbClf.predict(X_test)

# Compute accuracy
score_train = accuracy_score(y_train_clf,y_train_pred)
score_test = accuracy_score(y_test_clf,y_test_pred)

# Compute F1
train_f1=f1_score(y_train_clf,y_train_pred,average="weighted")
test_f1=f1_score(y_test_clf,y_test_pred,average="weighted")

# Adding scores
metrics_compare.append([model_name,"Train","Accuracy",round(score_train,4)])
metrics_compare.append([model_name,"Test","Accuracy",round(score_test,4)])
metrics_compare.append([model_name,"Train","F1",round(train_f1,4)])
metrics_compare.append([model_name,"Test","F1",round(test_f1,4)])


# save model for future use
filename = "Models/Classification/" + model_name + ".sav"
joblib.dump(xgbClf, filename)

"""# **Model Optimization**

The result of the model comparison shows, that the XGBoost for Regression and also for Classification gave us the smalles error and best accuracy respectively.

In this step we will try to optimize these models.

# **XGBoost Regressor**
"""

# XGBRegressor optimization
model_name = "XGBRegressor optimized"

params = {
    'learning_rate': [0.1, 0.01, 0.001],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 500, 1000]
}

xgbReg = XGBRegressor(objective='reg:squarederror')

# Perform GridSearch
gridcv = GridSearchCV(estimator=xgbReg, param_grid=params, cv=3, scoring='neg_mean_squared_error')
gridcv.fit(X_train, y_train_reg)

# Get the best parameters and best estimator
best_params = gridcv.best_params_
best_estimator = gridcv.best_estimator_

# Predict using the best estimator
y_train_reg_pred = best_estimator.predict(X_train)
y_test_reg_pred = best_estimator.predict(X_test)

# Compute R2
score_train = r2_score(y_train_reg,y_train_reg_pred)
score_test = r2_score(y_test_reg,y_test_reg_pred)

# Compute RMSE
train_rmse = np.sqrt(mean_squared_error(y_train_reg, y_train_reg_pred))
test_rmse = np.sqrt(mean_squared_error(y_test_reg, y_test_reg_pred))

print("\n")
print("Results for XGBRegressor")
print("-------------------------")
print("Best parameters:", best_params)

# Adding scores
metrics_compare.append([model_name,"Train","R^2",round(score_train,4)])
metrics_compare.append([model_name,"Test","R^2",round(score_test,4)])
metrics_compare.append([model_name,"Train","RMSE",round(train_rmse,4)])
metrics_compare.append([model_name,"Test","RMSE",round(test_rmse,4)])

# Feature importance
featimp_score = best_estimator.feature_importances_
col_names = X_train.columns
df_featimp = pd.DataFrame(featimp_score,index=col_names[:len(featimp_score)],columns=["importance"])
df_featimp = df_featimp.sort_values(by="importance")
df_featimp_10 = df_featimp.tail(10)
df_featimp_10

print("Feature importance")
print(df_featimp_10)
df_featimp_10.to_excel("xgbReg_Feature_importance.xlsx")

fig, ax = plt.subplots(1, 1, figsize=(6, 4))
ax.barh(range(len(df_featimp_10)), df_featimp_10['importance'], tick_label=df_featimp_10.index)
ax.set_xlabel('Feature Importance Percentage')
ax.set_ylabel('Feature')
ax.set_title('Feature Importance')
fig.tight_layout()
fig.savefig("xgbReg_Feature_importance.jpg")
plt.show();

# save model for future use
filename = "Models/Optimization/" + model_name + ".sav"
joblib.dump(gridcv, filename)


"""# **XGBoost Classifier**"""

# XGBClassifier optimization
model_name = "XGBClassifier optimized"

params = {
    'learning_rate': [0.1, 0.01, 0.001],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 500, 1000]
}

xgbClf = XGBClassifier(objective='multi:softmax')

# Perform GridSearch
gridcv = GridSearchCV(estimator=xgbClf, param_grid=params, cv=3, scoring='accuracy')
gridcv.fit(X_train, y_train_clf)

# Get the best parameters and best estimator
best_params = gridcv.best_params_
best_estimator = gridcv.best_estimator_

# Predict using the best estimator on validation set
y_train_clf_pred = best_estimator.predict(X_train)
y_test_clf_pred = best_estimator.predict(X_test)

# Compute accuracy
score_train = accuracy_score(y_train_clf, y_train_clf_pred)
score_test = accuracy_score(y_test_clf, y_test_clf_pred)

# Compute f1
train_f1=f1_score(y_train_clf,y_train_clf_pred,average="weighted")
test_f1=f1_score(y_test_clf,y_test_clf_pred,average="weighted")

print("\n")
print("Results for XGBClassifier")
print("-------------------------")
print("Best parameters:", best_params)

print("Classification report (Training)")
print(classification_report_imbalanced(y_train_clf, y_train_clf_pred))
print(classification_report(y_train_clf, y_train_clf_pred))

print("Classification report (Test)")
print(classification_report_imbalanced(y_test_clf, y_test_clf_pred))
print(classification_report(y_test_clf, y_test_clf_pred))

# Adding scores
metrics_compare.append([model_name,"Train","Accuracy",round(score_train,4)])
metrics_compare.append([model_name,"Test","Accuracy",round(score_test,4)])
metrics_compare.append([model_name,"Train","F1",round(train_f1,4)])
metrics_compare.append([model_name,"Test","F1",round(test_f1,4)])

# Feature importance
featimp_score = best_estimator.feature_importances_
col_names = X_train.columns
df_featimp = pd.DataFrame(featimp_score,index=col_names[:len(featimp_score)],columns=["importance"])
df_featimp = df_featimp.sort_values(by="importance")
df_featimp_10 = df_featimp.tail(10)

print("Feature importance")
print(df_featimp_10)
df_featimp_10.to_excel("xgbClf_Feature_importance.xlsx")

fig, ax = plt.subplots(1, 1, figsize=(6, 4))
ax.barh(range(len(df_featimp_10)), df_featimp_10['importance'], tick_label=df_featimp_10.index)
ax.set_xlabel('Feature Importance Percentage')
ax.set_ylabel('Feature')
ax.set_title('Feature Importance')
fig.tight_layout()
fig.savefig("xgbClf_Feature_importance.jpg")
plt.show();


# save model for future use
filename = "Models/Optimization/" + model_name + ".sav"
joblib.dump(gridcv, filename)

"""# **Neural Networks**

In the following steps we will train a neural network for the same problematic as before.

# **Import libraries**
"""

#for modelling NN
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model

#for dynamically changes during the training
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

#for optimization with GridSearchCV
from scikeras.wrappers import KerasClassifier

#for optimizer definitions with individual learning-rates( NN regression could have often problems with exploding-gradients )
from tensorflow import keras

"""# **Preprocessing**"""

input_len = len(X_train.columns)
output_len =  len(y_train_clf.unique())
print('Number of features:' , input_len)
print('Number of classes:', output_len)

inputs = Input(shape = (input_len,), name = "Input")

dense1 = Dense(units = 166, activation = 'tanh', name = 'Dense_1')

dense2 = Dense(units = 332, activation = 'tanh', name = 'Dense_2')

dense3 = Dense(units = 166, activation = 'tanh', name = 'Dense_3')

dense4 = Dense(units = 166, activation = 'tanh', name = 'Dense_4')

dense5_clf = Dense(units = output_len, activation = 'softmax', name = 'Dense_5_clf')

dense5_reg = Dense(units = 1, activation = 'linear', name = 'Dense_5_reg')

early_stopping = EarlyStopping(monitor = 'val_loss',
                               patience = 5,
                               min_delta = 0.01,
                               mode = 'min',
                               verbose = 1
                              )

reduce_learning_rate = ReduceLROnPlateau(monitor = 'val_loss',
                                         factor = 0.1,
                                         patience = 3,
                                         min_delta = 0.01,
                                         cooldown = 4,
                                         verbose = 1
                                        )

"""# **Regression**"""

# Regression

x = dense1(inputs)

x = dense2(x)

x = dense3(x)

x = dense4(x)

outputs = dense5_reg(x)

model_reg = Model(inputs = inputs, outputs = outputs)

model_reg.compile(loss = 'mse',
                  optimizer = 'adam',
                  metrics = ['mean_absolute_error'])

model_reg.summary()

model_history_reg = model_reg.fit(X_train,
                              y_train_reg,
                              epochs=15,
                              batch_size=512,
                              validation_data=(X_valid,y_valid_reg),
                              callbacks = [reduce_learning_rate, early_stopping])

train_loss = model_history_reg.history["loss"]
val_loss = model_history_reg.history["val_loss"]
train_acc = model_history_reg.history["mean_absolute_error"]
val_acc = model_history_reg.history["val_mean_absolute_error"]

# Plot 1: Loss per epoch
plt.figure(figsize=(10, 8))
plt.plot(train_loss)
plt.plot(val_loss)
plt.title('Model loss per epoch (MSE)')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.savefig("NN_Regression_loss_plot.jpg")
plt.show()

# Plot 2: Accuracy per epoch
plt.figure(figsize=(10, 8))
plt.plot(train_acc)
plt.plot(val_acc)
plt.title('Model accuracy per epoch (MAE)')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.savefig("NN_Regression_accuracy_plot.jpg")
plt.show()

# Predictions
y_train_reg_pred = model_reg.predict(X_train)
y_test_reg_pred = model_reg.predict(X_test)

# Compute R2
score_train=r2_score(y_train_reg,y_train_reg_pred)
score_test=r2_score(y_test_reg,y_test_reg_pred)

# Compute RMSE
train_rmse = np.sqrt(mean_squared_error(y_train_reg,y_train_reg_pred))
test_rmse = np.sqrt(mean_squared_error(y_test_reg,y_test_reg_pred))

# Adding scores
model_name= "NN Regression"

metrics_compare.append([model_name,"Train","R^2",round(score_train,4)])
metrics_compare.append([model_name,"Test","R^2",round(score_test,4)])
metrics_compare.append([model_name,"Train","RMSE",round(train_rmse,4)])
metrics_compare.append([model_name,"Test","RMSE",round(test_rmse,4)])


# save model for future use
filename = "Models/NN/" + model_name + ".sav"
joblib.dump(model_reg, filename)

"""# **Classification**"""

# Classification

x = dense1(inputs)

x = dense2(x)

x = dense3(x)

x = dense4(x)

outputs = dense5_clf(x)

model_clf = Model(inputs = inputs, outputs = outputs)

model_clf.compile(loss = 'sparse_categorical_crossentropy',
                  optimizer = 'adam',
                  metrics = ['accuracy'])

model_clf.summary()

model_history_clf = model_clf.fit(X_train,
                              y_train_clf,
                              epochs=15,
                              batch_size=512,
                              validation_data=(X_valid,y_valid_clf),
                              callbacks = [reduce_learning_rate, early_stopping])

train_loss = model_history_clf.history["loss"]
val_loss = model_history_clf.history["val_loss"]
train_acc = model_history_clf.history["accuracy"]
val_acc = model_history_clf.history["val_accuracy"]

# Plot 1: Loss per epoch
plt.figure(figsize=(10, 8))
plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Model Loss per Epoch')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.tight_layout()
plt.savefig("NN_Classification_loss_plot.jpg")
plt.show()

# Plot 2: Accuracy per epoch
plt.figure(figsize=(10, 8))
plt.plot(train_acc, label='Train Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Model Accuracy per Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.tight_layout()
plt.savefig("NN_Classification_accuracy_plot.jpg")
plt.show()

# Prediction (training set)
y_train_clf_pred = model_clf.predict(X_train);
y_train_clf_pred_class = np.argmax(y_train_clf_pred, axis=1)

print("NN Classification report (Training):")
print(classification_report(y_train_clf, y_train_clf_pred_class))

# Prediction (test set)
y_test_clf_pred = model_clf.predict(X_test);
y_test_clf_pred_class = np.argmax(y_test_clf_pred, axis=1)

print("NN Classification report (Test):")
print(classification_report(y_test_clf, y_test_clf_pred_class))
print(confusion_matrix(y_test_clf, y_test_clf_pred_class))

# Compute accuracy
score_train = accuracy_score(y_train_clf, y_train_clf_pred_class)
score_test = accuracy_score(y_test_clf, y_test_clf_pred_class)

# Compute f1
train_f1=f1_score(y_train_clf,y_train_clf_pred_class,average="weighted")
test_f1=f1_score(y_test_clf,y_test_clf_pred_class,average="weighted")

# Adding scores
model_name= "NN Classification"

metrics_compare.append([model_name,"Train","Accuracy",round(score_train,4)])
metrics_compare.append([model_name,"Test","Accuracy",round(score_test,4)])
metrics_compare.append([model_name,"Train","F1",round(train_f1,4)])
metrics_compare.append([model_name,"Test","F1",round(test_f1,4)])

# save model for future use
filename = "Models/NN/" + model_name + ".sav"
joblib.dump(model_clf, filename)

"""# **Optimization**

# **NN Classification Optimization**
"""

# create_model for classification
def create_model(optimizer='Adam'):

    x=dense1(inputs)
    #x=dropout(x)
    x=dense2(x)
    #x=dropout(x)
    x=dense3(x)
    #x=dropout(x)
    x=dense4(x)
    outputs=dense5_clf(x)

    model=Model(inputs= inputs, outputs = outputs)
    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])
    return model

model_keras_clf = KerasClassifier(model=create_model,
                                  epochs=100,
                                  batch_size=8192,
                                  callbacks = [reduce_learning_rate, early_stopping]) #, verbose=0)

# define of hyperparameters
optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
batchsizes = [512,256,128]
parameters = dict(optimizer=optimizers,batch_size=batchsizes)

# define the GridSearchCV
grid_clf = GridSearchCV(estimator=model_keras_clf, param_grid=parameters,cv=3,verbose=10) #, n_jobs=-1, cv=2)

# train the GridSearchCV
grid_result = grid_clf.fit(X_train,
                           y_train_clf,
                           validation_data=(X_valid,y_valid_clf))

train_loss = grid_clf.best_estimator_.history_["loss"]
val_loss = grid_clf.best_estimator_.history_["val_loss"]
train_acc = grid_clf.best_estimator_.history_["accuracy"]
val_acc = grid_clf.best_estimator_.history_["val_accuracy"]

# Plot 1: Loss per epoch
plt.figure(figsize=(10, 8))
plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Model Loss per Epoch')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.tight_layout()
plt.savefig("NN_Classification_best_gridcv_loss_plot.jpg")
plt.show()

# Plot 2: Accuracy per epoch
plt.figure(figsize=(10, 8))
plt.plot(train_acc, label='Train Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Model Accuracy per Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.tight_layout()
plt.savefig("NN_Classification_best_gridcv_accuracy_plot.jpg")
plt.show()

# Prediction (training set)
y_train_clf_pred = grid_result.predict(X_train);

print("NN Classification report (Training):")
print(classification_report(y_train_clf, y_train_clf_pred))

# Prediction (test set)
y_test_clf_pred = grid_result.predict(X_test);
#y_test_clf_pred_class = np.argmax(y_test_clf_pred, axis=1)

print("NN Classification report (Test):")
print(classification_report(y_test_clf, y_test_clf_pred))
print(confusion_matrix(y_test_clf, y_test_clf_pred))

# Compute accuracy
score_train = accuracy_score(y_train_clf, y_train_clf_pred)
score_test = accuracy_score(y_test_clf, y_test_clf_pred)

# Compute f1
train_f1=f1_score(y_train_clf,y_train_clf_pred,average="weighted")
test_f1=f1_score(y_test_clf,y_test_clf_pred,average="weighted")

# Adding scores
best_param= grid_clf.best_params_
model_name= 'NN Classification optimized(batchsize=%s, optimizer=%s)' % (best_param['batch_size'],best_param['optimizer'])

metrics_compare.append([model_name,"Train","Accuracy",round(score_train,4)])
metrics_compare.append([model_name,"Test","Accuracy",round(score_test,4)])
metrics_compare.append([model_name,"Train","F1",round(train_f1,4)])
metrics_compare.append([model_name,"Test","F1",round(test_f1,4)])


# save model for future use
try:
    filename = "Models/Optimization/NN Classification optimized.pkl"
    joblib.dump(grid_clf.best_estimator_, filename)
except:
    print("Error occured during saving NN Classification optimized model!")

"""# **NN Regression Optimization**"""

# create_model for regression
def create_model_reg(optimizer='Adam'):

    x=dense1(inputs)
    x=dense2(x)
    x=dense3(x)
    x=dense4(x)
    outputs=dense5_reg(x)

    model=Model(inputs= inputs, outputs = outputs)
    model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mean_absolute_error'])
    return model

model_keras_reg = KerasClassifier(model=create_model_reg,
                                  epochs=100,
                                  batch_size=8192,
                                  callbacks = [reduce_learning_rate, early_stopping]) #, verbose=0)

# define of hyperparameters
optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
batchsizes = [512,256,128]
parameters = dict(optimizer=optimizers,batch_size=batchsizes)

# define the GridSearchCV
grid_reg = GridSearchCV(estimator=model_keras_reg, param_grid=parameters,cv=3,verbose=10) #, n_jobs=-1)

# train the GridSearchCV
grid_result = grid_reg.fit(X_train,
                           y_train_clf,
                           validation_data=(X_valid,y_valid_clf))

train_loss = grid_reg.best_estimator_.history_["loss"]
val_loss = grid_reg.best_estimator_.history_["val_loss"]
train_acc = grid_reg.best_estimator_.history_["mean_absolute_error"]
val_acc = grid_reg.best_estimator_.history_["val_mean_absolute_error"]

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

ax1.plot(train_loss)
ax1.plot(val_loss)
ax1.set_title('Model loss per epoch (MSE)')
ax1.set_ylabel('loss')
ax1.set_xlabel('epoch')
ax1.legend(['train', 'valid'], loc='right')

ax2.plot(train_acc)
ax2.plot(val_acc)
ax2.set_title('Model accuracy per epoch (MAE)')
ax2.set_ylabel('acc')
ax2.set_xlabel('epoch')
ax2.legend(['train', 'valid'], loc='right')
fig.tight_layout()
fig.savefig("NN Regression_loss_plot.jpg")
plt.show();

# Predictions
y_train_reg_pred = grid_result.predict(X_train)
y_test_reg_pred = grid_result.predict(X_test)

# Compute R2
score_train=r2_score(y_train_reg,y_train_reg_pred)
score_test=r2_score(y_test_reg,y_test_reg_pred)

# Compute RMSE
train_rmse = np.sqrt(mean_squared_error(y_train_reg,y_train_reg_pred))
test_rmse = np.sqrt(mean_squared_error(y_test_reg,y_test_reg_pred))

# Adding scores
best_param= grid_reg.best_params_
model_name= model_name='NN Regression optimized(batchsize=%s, optimizer=%s)' % (best_param['batch_size'],best_param['optimizer'])

metrics_compare.append([model_name,"Train","R^2",round(score_train,4)])
metrics_compare.append([model_name,"Test","R^2",round(score_test,4)])
metrics_compare.append([model_name,"Train","RMSE",round(train_rmse,4)])
metrics_compare.append([model_name,"Test","RMSE",round(test_rmse,4)])

# save model for future use
try:
    filename = "Models/Optimization/NN Regression optimized.pkl"
    joblib.dump(grid_reg.best_estimator_, filename)
except:
    print("Error occured during saving NN Regression optimized model!")

#  for comparing results
df_metrics_compare = pd.DataFrame(metrics_compare,columns=["Model","Set","metric","score"])

print("Modeling: Finished")
print("Summery of the statistics:")
print(df_metrics_compare)

df_metrics_compare.to_excel("metrics.xlsx")

"""# **Interpretation**"""

from lime.lime_tabular import LimeTabularExplainer

# Initialize the explainer
explainer = LimeTabularExplainer(X_test.values, mode='classification', feature_names=X_test.columns, verbose=True, discretize_continuous=True)

# Define the function to explain
def proba_func(X):
  proba = xgbClf.predict_proba(X)
  return proba

# Choose an instance to explain
instance_index = 3445
instance = X_test.iloc[instance_index].values.reshape(1, -1)

# Explain the instance
explanation = explainer.explain_instance(instance[0], proba_func, num_features=10)

# Show the explanation
explanation.show_in_notebook();

